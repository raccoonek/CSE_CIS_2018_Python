# CSE_CIS_2018_Python

Ссылка на датасет: https://drive.google.com/file/d/18zYaSoMC0i7H3t-wJMZBC_hs1D72MDKX/view?usp=sharing


Результаты работы моделей.
Логистическая регрессия
В исследовании была применена модель логистической регрессии с L2-регуляризацией (ридж-регрессия). Коэффициент обратной силы регуляризации C был принят равным 1, что обеспечивает баланс между точностью модели и ее регуляризацией. 
Оптимизация функции потерь осуществлялась с помощью алгоритма L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno), который демонстрирует высокую эффективность для задач среднего масштаба. Данный метод относится к квазиньютоновским алгоритмам и сочетает в себе преимущества устойчивости и вычислительной эффективности.
Критерий остановки оптимизации был установлен на уровне 1e-4 по значению градиента функции потерь, что обеспечивает достаточную точность решения при разумных вычислительных затратах. Вопрос дисбаланса классов решался с помощью автоматической балансировки весов ('balanced'), когда веса классов обратно пропорциональны их частотам в входных данных, что особенно важно при работе с неравномерно распределенными выборками.
Результаты предсказания на тестовой выборке представлены на рисунке 4.

Рисунок 4. Матрица ошибок для логистической регрессии.
Результаты вычисленных метрик:
Training Time: 70.0124 seconds
Peak Memory Usage: 7083.28 MB
CPU Usage: 14.8%
Prediction Time: 0.1640 seconds
Classification Report:


|        |precision| recall| f1-score| support |
|--------|---------|-------|---------|---------|
|Anomaly | 0.91383 |0.86329|0.88798  | 650503  |
|Normal  |0.95213  |0.97093|0.96143  |1822073  |
         
Accuracy:   0.94262 

Дерево решений (CART)
В данном исследовании применялся алгоритм CART (Classification and Regression Trees) для построения модели дерева решений с использованием критерия Джини, как меры качества разбиения.
Результаты предсказания на тестовой выборке представлены на рисунке 5.
Результаты вычисленных метрик:
Training Time: 274.0303 seconds
Peak Memory Usage: 7085.51 MB
CPU Usage: 14.0%
Prediction Time: 0.8619 seconds
Classification Report:


precision
 recall 
f1-score
support
Anomaly
0.94099
0.93783  
0.93941 
 650727
Normal
0.97782 
 0.97899  
 0.9784 
1821849


Accuracy: 0.96816
 
Рисунок 5. Матрица ошибок для дерева решений.

Случайный лес
В настоящем исследовании была реализована ансамблевая модель случайного леса, состоящая из 20 решающих деревьев (n_estimators=20).
Каждое дерево в ансамбле строилось с использованием алгоритма CART (Classification and Regression Trees). Результаты предсказания на тестовой выборке представлены на рисунке 6.

Рисунок 6. Матрица ошибок для случайного леса.
Результаты вычисленных метрик:
Training Time: 456.8660 seconds
Peak Memory Usage: 7434.53 MB
CPU Usage: 14.9%
Prediction Time: 13.0541 seconds
Classification Report:


precision
 recall 
f1-score
support
Anomaly
0.97078
 0.93406
 0.95197
 650727
Normal
0.97676 
 0.98996  
 0.9833
1821849





Accuracy:  0.97525

Бустинг (Boosting trees)
В рамках данного исследования была реализована модель градиентного бустинга (Boosting Trees) со следующими параметрами конфигурации:
Ансамбль состоит из 20 последовательно обучаемых деревьев решений (n_estimators=20), где каждое последующее дерево корректирует ошибки предыдущих. Скорость обучения (learning_rate) установлена на уровне 0.1, что обеспечивает плавную оптимизацию модели и способствует достижению лучшего обобщения.
В качестве функции потерь выбрана логистическая функция ('log_loss'), которая оптимально подходит для задач классификации и вычисляет отрицательное логарифмическое правдоподобие. Формально функция потерь определяется как:
L(y,ŷ) = -[y·log(ŷ) + (1-y)·log(1-ŷ)]
, где 
Каждое дерево в ансамбле ограничено максимальной глубиной 3 (max_depth=3), что создает так называемые "слабые обучатели" (weak learners) и предотвращает переобучение. 
Результаты предсказания на тестовой выборке представлены на рисунке 7.

Рисунок 7. Матрица ошибок для Boosting trees.
Результаты вычисленных метрик:
Training Time: 713.7391 seconds
Peak Memory Usage: 7082.26 MB
CPU Usage: 17.1%
Prediction Time: 1.4227 seconds
Classification Report:


precision
 recall 
f1-score
support
Anomaly
0.99378
 0.9152
 0.95295
  650727
Normal
 0.97054 
 0.99795
 0.98397
1821849


              
      

    Accuracy: 0.9762 

Метод k-ближайших соседей (knn)
В рамках исследования был применен непараметрический метод k-ближайших соседей (k-NN), характеризующийся следующими ключевыми параметрами настройки. Основной управляющий параметр алгоритма - количество учитываемых соседей (n_neighbors) - было установлено равным 5. Данное значение представляет собой оптимальный компромисс между чувствительностью модели к локальным особенностям распределения данных (при меньших значениях) и устойчивостью к шумовым выбросам (при больших значениях).
Для эффективного поиска ближайших соседей в пространстве признаков был выбран алгоритм BallTree ('ball_tree'), который особенно эффективен для данных высокой размерности. Этот метод строит древовидную структуру данных, позволяющую значительно сократить вычислительные затраты по сравнению с полным перебором. Размер листа (leaf_size) был установлен на уровне 30 (по умолчанию), что оптимизирует баланс между скоростью построения дерева и точностью поиска ближайших соседей.
В качестве метрики расстояния между точками использовалась обобщенная метрика Минковского (metric='minkowski') с параметром p=2, что соответствует классическому евклидову расстоянию. Данная метрика вычисляется по формуле:
d(x,y) = (Σ|x_i - y_i|p)(1/p)
,где при p=2 получается стандартное евклидово расстояние, хорошо работающее во многих практических задачах. 
Результаты предсказания на тестовой выборке представлены на рисунке 8.

Рисунок 8. Матрица ошибок для метода k-ближайших соседей.
Результаты вычисленных метрик:
Training Time: 1.1503 seconds
Peak Memory Usage: 9087.78 MB
CPU Usage: 21.7%
Prediction Time: 12583.8754 seconds
Classification Report:


precision
 recall 
f1-score
support
Anomaly
0.98725
 0.93291
 0.9593
 650727
Normal
0.9765
 0.99897 
 0.9876
1821849


            

    Accuracy: 0.9792

Метод опорных векторов (svc)
В данном исследовании применялась линейная версия метода опорных векторов (SVM), реализованная через стохастический градиентный спуск со следующими параметрами оптимизации. В качестве функции потерь использовался hinge loss (loss='hinge'), который формально определяется как:
 L(y) = max(0, 1 - t·y) 
, где t - истинный класс, а y - решающая функция. Этот выбор обеспечивает построение классификатора с максимальным зазором, что соответствует канонической формулировке SVM.
Для обеспечения сходимости алгоритма установлено предельное количество итераций max_iter=10⁷, что гарантирует достижение оптимума даже для сложноразделимых данных. Регуляризация модели осуществляется комбинированным L1/L2-методом (penalty='elasticnet') с коэффициентом α=0.0001, где доля L1-составляющей составляет 15% (l1_ratio=0.15).
Результаты предсказания на тестовой выборке представлены на рисунке 9.

Рисунок 9. Матрица ошибок для метода опорных векторов.
Результаты вычисленных метрик:
Training Time: 12.1104 seconds
Peak Memory Usage: 7052.06 MB
CPU Usage: 15.3%
Prediction Time: 0.1645 seconds
Classification Report:


precision
 recall 
f1-score
support
Anomaly
0.8748
 0.86511
 0.86994
 650727
Normal
0.95202
 0.9558
 0.9539
1821849


                 

    Accuracy:  0.93194

Нейронная сеть

В исследовании была реализована многослойная нейронная сеть прямого распространения (MLP) со следующей архитектурой и параметрами обучения. Сетевая структура состоит из двух скрытых слоев: первый слой содержит 1024 нейрона, второй - 128 нейронов (hidden_layer_sizes=(1024, 128)). Такая конфигурация позволяет первому слою выделять сложные признаки из входных данных, в то время как второй слой осуществляет их эффективную компрессию, формируя компактное представление информации перед классификацией.
В качестве функции активации скрытых слоев выбрана выпрямляющая линейная единица (ReLU, activation='relu'), определяемая как f(x) = max(0, x). 
Оптимизация весов сети осуществляется с помощью алгоритма Adam (solver='adam'), который сочетает преимущества методов Momentum и RMSProp. Данный оптимизатор автоматически адаптирует шаг обучения для каждого параметра, используя оценки первого и второго моментов градиентов. Начальная скорость обучения установлена на уровне 0.01 (learning_rate_init=0.01), что является стандартным диапазоном для Adam.
Результаты предсказания на тестовой выборке представлены на рисунке 10.
Результаты вычисленных метрик:
Training Time: 3629.4615 seconds
Peak Memory Usage: 7090.39 MB
CPU Usage: 22.1%
Prediction Time: 41.0420 seconds
Classification Report:


precision
 recall 
f1-score
support
Anomaly
0.99208
 0.9262
 0.95812
  650727
Normal
0.97427
 0.99736
 0.98568
1821849






Accuracy:  0.97864   
 

Рисунок 10. Матрица ошибок для нейронной сети.

В. Сравнительный анализ полученных результатов.
Для комплексной оценки эффективности моделей в условиях дисбаланса классов (преобладания нормального трафика над аномальным) был проведен детальный анализ метрик precision, recall и F1-score отдельно для каждого класса.
Результаты классификации аномального трафика (табл. 3) отражают способность моделей детектировать угрозы в условиях их малой представленности в данных.

Таблица 3. Метрики для аномального трафика
Модель
Метрики
precision
recall
f1-score
Логистическая регрессия
0.91383
0.86329
0.88798
Дерево решений 
0.94099
0.93783
0.93941
Случайный лес
0.97078
0.93406
0.95197
Boosting trees
0.99378
0.9152
0.95295
knn
0.98725
0.93291
0.9593
svc
0.87487
0.86511
0.86994
Нейронная сеть
0.99208
0.92625
0.95812

Precision (точность):
Наивысшие значения у Boosting Trees (0.99378) и нейронной сети (0.99208). Это означает, что данные модели редко ошибочно классифицируют нормальный трафик как аномальный, что особенно ценно для минимизации ложных срабатываний в системах безопасности. 
Логистическая регрессия (0.91383) и SVC (0.87487) показали более низкие результаты, вероятно, из-за линейного характера, не способного уловить сложные нелинейные зависимости.
Recall (полнота):
Дерево решений (0.93783) и Случайный лес (0.93406) лидируют, обнаруживая более 93% аномалий. Это делает их пригодными для задач, где пропуск угроз недопустим.
Boosting Trees (0.9152) и нейронная сеть (0.92625) слегка уступают, но компенсируют это высокой точностью, это указывает на возможные проблемы с обнаружением новых, ранее не встречавшихся типов атак.
F1-score:
KNN (0.9593) и Случайный лес (0.95197) демонстрируют наилучший баланс между precision и recall.
Логистическая регрессия (0.88798) и SVC (0.86994) значительно отстают, что указывает на их ограниченную применимость для сложных данных.
Результаты классификации нормального трафика (табл. 4) особенно важны для понимания способности алгоритмов минимизировать ложные срабатывания. Все методы показали высокие значения F1-меры (>0.95), что свидетельствует о корректности проведенной предобработки данных и настройки моделей.
Таблица 4. Метрики для нормального трафика
Модель
Метрики
precision
recall
f1-score
Логистическая регрессия
0.95213
0.97093
0.96143
Дерево решений
0.97782
0.97899
0.9784
Случайный лес
0.97676
0.98996
0.9833
Boosting trees
0.97054
0.99795
0.98397
knn
0.9765
0.99897
0.9876
svc
0.95202
0.9558
0.9539
Нейронная сеть
0.97427
0.99736
0.98568

Precision:
Дерево решений (0.97782) и Случайный лес (0.97676) показывают наименьшее количество ложных тревог для нормального трафика.
KNN (0.9765) и нейронная сеть (0.97427) следуют за ними с сопоставимыми результатами.
Recall:
KNN (0.99897) и Boosting Trees (0.99795) почти идеально распознают нормальные образцы, минимизируя ложноотрицательные срабатывания.
Нейронная сеть (0.99736) также демонстрирует выдающуюся полноту.
F1-score:
KNN (0.9876) и Boosting Trees (0.98397) лидируют, подтверждая свою эффективность в условиях доминирования нормального трафика.
Анализ вычислительной производительности (табл. 5) выявил зависимость между сложностью алгоритма и его ресурсоемкостью. 
Таблица 5. Сравнение вычислительной производительности алгоритмов
алгоритм / метрика
accuracy
Training Time (s)
Peak Memory Usage (MB)
CPU Usage (%)
Prediction Time (s)




Логистическая регрессия
0.94262
70.0124
7083.28
14.8
0.164


Дерево решений
0.96816
274.0303
7085.51
14
0.8619


Случайный лес
0.97525
456.866
7434.53
14.9
13.0541


Boosting trees
0.9762
713.7391
7082.26
17.1
1.4227


knn
0.9792
1.1503
9087.78
21.7
12583.88


svc
0.93194
12.1104
7052.06
15.3
0.1645


Нейронная сеть
0.97864
3629.46
7090.39
22.1
41.042




Accuracy (Общая точность)
KNN (0.9792) достигает максимальной точности, но его время предсказания (12583.88 сек) делает его непригодным для реального времени.
Boosting Trees (0.9762) и нейронная сеть (0.97864) близки к KNN по точности, но с более приемлемым временем предсказания.
Время обучения
Логистическая регрессия (70.01 сек) и SVC (12.11 сек) обучаются быстрее всего благодаря простой структуре.
Нейронная сеть (3629.46 сек) и Boosting Trees (713.74 сек) требуют значительных временных затрат, что ограничивает их применение в сценариях частого переобучения.
Использование памяти
KNN (9087.78 МБ) потребляет наибольший объем памяти из-за хранения всего обучающего набора, что проблематично для больших данных.
Остальные модели используют около 7000 МБ, что приемлемо для большинства систем.
Загрузка CPU
Нейронная сеть (22.1%) и KNN (21.7%) наиболее требовательны к CPU.
Логистическая регрессия (14.8%) и SVC (15.3%) оптимальны для систем с ограниченными ресурсами.
Время предсказания
Логистическая регрессия (0.164 сек) и SVC (0.1645 сек) подходят для обработки данных в реальном времени, это делает их идеальными для встраиваемых систем (например, межсетевых экранов), где важна минимальная задержка. .
KNN (12583 сек) неприменима в сценариях, требующих мгновенного ответа.

На основе полученных результатов приведем сильные и слабые стороны исследуемых алгоритмов и их возможные сферы применения (табл. 6).
Таблица 6. Сравнение алгоритмов
Модель
Сильные стороны
Слабые стороны
Применение
Логистическая регрессия
Быстрое обучение/предсказание, низкая нагрузка на систему
Низкий recall для аномалий
Базовые системы мониторинга в реальном времени
Дерево решений
Высокий recall аномалий, простота реализации
Склонность к переобучению на шумных данных, средняя общая точность.
Экспресс-анализ
Случайный лес
Сбалансированность precision и recall, устойчивость к шуму за счёт ансамблирования.
Высокое время предсказания
Оффлайн-анализ, задачи с требованием высокой точности и стабильности.
Boosting Trees
Максимальная точность, минимальные ложные срабатывания.
Ресурсоемкость и длительное время обучения из-за последовательного построения моделей.
Системы, где критично ложные срабатывания.
KNN
Лучшие результаты предсказаний
Экстремальное  время предсказания, объем потребляемой памяти
Тестовые среды, небольшие наборы данных. Непригоден для промышленного внедрения.
SVC
Умеренная точность и быстрое предсказание
Низкая эффективность для аномалий из-за чувствительности к дисбалансу классов.
Сценарии, требующие баланса между скоростью и точностью, но без жёстких требований к recall.
Нейронная сеть
Высокая адаптивность к сложным данным, точность, близкая к ансамблевым методам
Длительное обучение и высокие требования к вычислительным ресурсам (CPU/GPU).
Сложные задачи с большими объёмами данных, где допустимы затраты на обучение и оптимизацию.


